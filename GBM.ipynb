{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgbm\n",
    "import optuna as opt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gc\n",
    "import shap\n",
    "from scipy.stats import binom_test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/matthewlafferty/Dropbox/Kaggle/Tabular_Feb_2021/tabular-playground-series-feb-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>262658</th>\n",
       "      <td>437948</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244489</td>\n",
       "      <td>0.689357</td>\n",
       "      <td>0.637401</td>\n",
       "      <td>0.483618</td>\n",
       "      <td>0.297313</td>\n",
       "      <td>0.283682</td>\n",
       "      <td>0.852316</td>\n",
       "      <td>0.285454</td>\n",
       "      <td>0.701334</td>\n",
       "      <td>7.515493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113751</th>\n",
       "      <td>189563</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583045</td>\n",
       "      <td>0.930331</td>\n",
       "      <td>0.300816</td>\n",
       "      <td>0.127790</td>\n",
       "      <td>0.224117</td>\n",
       "      <td>0.672991</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>0.358266</td>\n",
       "      <td>0.775926</td>\n",
       "      <td>6.509431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190510</th>\n",
       "      <td>317720</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.729128</td>\n",
       "      <td>0.926942</td>\n",
       "      <td>0.880656</td>\n",
       "      <td>0.810298</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>0.799923</td>\n",
       "      <td>0.753010</td>\n",
       "      <td>0.822090</td>\n",
       "      <td>6.647751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54397</th>\n",
       "      <td>90493</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293556</td>\n",
       "      <td>0.281192</td>\n",
       "      <td>0.324031</td>\n",
       "      <td>0.051493</td>\n",
       "      <td>0.182182</td>\n",
       "      <td>0.231336</td>\n",
       "      <td>0.176297</td>\n",
       "      <td>0.413605</td>\n",
       "      <td>0.307887</td>\n",
       "      <td>7.423009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99124</th>\n",
       "      <td>165106</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381412</td>\n",
       "      <td>0.427531</td>\n",
       "      <td>0.339106</td>\n",
       "      <td>0.424547</td>\n",
       "      <td>0.446630</td>\n",
       "      <td>0.652907</td>\n",
       "      <td>0.337916</td>\n",
       "      <td>0.374758</td>\n",
       "      <td>0.243413</td>\n",
       "      <td>7.222037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167945</th>\n",
       "      <td>279929</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495404</td>\n",
       "      <td>0.629675</td>\n",
       "      <td>0.281271</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>0.831618</td>\n",
       "      <td>0.710934</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.693807</td>\n",
       "      <td>0.269952</td>\n",
       "      <td>8.311413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96789</th>\n",
       "      <td>161220</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734606</td>\n",
       "      <td>0.420961</td>\n",
       "      <td>0.490053</td>\n",
       "      <td>0.368205</td>\n",
       "      <td>0.802769</td>\n",
       "      <td>0.714048</td>\n",
       "      <td>0.354470</td>\n",
       "      <td>0.766466</td>\n",
       "      <td>0.696809</td>\n",
       "      <td>7.667045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110668</th>\n",
       "      <td>184265</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373630</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.284551</td>\n",
       "      <td>0.344584</td>\n",
       "      <td>0.776556</td>\n",
       "      <td>0.609952</td>\n",
       "      <td>0.567342</td>\n",
       "      <td>0.713616</td>\n",
       "      <td>0.419782</td>\n",
       "      <td>7.550896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130926</th>\n",
       "      <td>218168</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950352</td>\n",
       "      <td>0.697246</td>\n",
       "      <td>0.752149</td>\n",
       "      <td>0.864242</td>\n",
       "      <td>0.807975</td>\n",
       "      <td>0.546746</td>\n",
       "      <td>0.531759</td>\n",
       "      <td>0.841601</td>\n",
       "      <td>0.302457</td>\n",
       "      <td>8.068698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245046</th>\n",
       "      <td>408735</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385891</td>\n",
       "      <td>0.317842</td>\n",
       "      <td>0.578686</td>\n",
       "      <td>0.422686</td>\n",
       "      <td>0.489252</td>\n",
       "      <td>0.576573</td>\n",
       "      <td>0.651665</td>\n",
       "      <td>0.358783</td>\n",
       "      <td>0.284274</td>\n",
       "      <td>5.174548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont5  \\\n",
       "262658  437948    B    A    A    A    B    B    A    E    C  ...  0.244489   \n",
       "113751  189563    A    A    A    C    B    D    A    E    C  ...  0.583045   \n",
       "190510  317720    A    A    A    A    B    D    A    E    E  ...  0.862581   \n",
       "54397    90493    A    B    B    C    B    B    A    E    E  ...  0.293556   \n",
       "99124   165106    A    A    A    C    B    B    A    E    C  ...  0.381412   \n",
       "167945  279929    A    A    A    C    B    D    A    E    C  ...  0.495404   \n",
       "96789   161220    A    B    B    A    B    D    A    E    E  ...  0.734606   \n",
       "110668  184265    A    B    A    C    B    D    A    E    C  ...  0.373630   \n",
       "130926  218168    A    B    A    A    B    D    A    B    G  ...  0.950352   \n",
       "245046  408735    A    A    A    D    B    B    A    E    C  ...  0.385891   \n",
       "\n",
       "           cont6     cont7     cont8     cont9    cont10    cont11    cont12  \\\n",
       "262658  0.689357  0.637401  0.483618  0.297313  0.283682  0.852316  0.285454   \n",
       "113751  0.930331  0.300816  0.127790  0.224117  0.672991  0.351220  0.358266   \n",
       "190510  0.729128  0.926942  0.880656  0.810298  0.849737  0.799923  0.753010   \n",
       "54397   0.281192  0.324031  0.051493  0.182182  0.231336  0.176297  0.413605   \n",
       "99124   0.427531  0.339106  0.424547  0.446630  0.652907  0.337916  0.374758   \n",
       "167945  0.629675  0.281271  0.441942  0.831618  0.710934  0.712174  0.693807   \n",
       "96789   0.420961  0.490053  0.368205  0.802769  0.714048  0.354470  0.766466   \n",
       "110668  0.464400  0.284551  0.344584  0.776556  0.609952  0.567342  0.713616   \n",
       "130926  0.697246  0.752149  0.864242  0.807975  0.546746  0.531759  0.841601   \n",
       "245046  0.317842  0.578686  0.422686  0.489252  0.576573  0.651665  0.358783   \n",
       "\n",
       "          cont13    target  \n",
       "262658  0.701334  7.515493  \n",
       "113751  0.775926  6.509431  \n",
       "190510  0.822090  6.647751  \n",
       "54397   0.307887  7.423009  \n",
       "99124   0.243413  7.222037  \n",
       "167945  0.269952  8.311413  \n",
       "96789   0.696809  7.667045  \n",
       "110668  0.419782  7.550896  \n",
       "130926  0.302457  8.068698  \n",
       "245046  0.284274  5.174548  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_convert(X:pd.Series) -> pd.Series:\n",
    "    temp = X.astype('category').cat.codes\n",
    "    temp += 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shadow_feature(X:pd.Series) -> np.array:\n",
    "    temp = X.to_numpy(copy=True)\n",
    "    np.random.shuffle(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data frame that we will use to store our SHAP value information\n",
    "results_df = pd.DataFrame(columns = shadow_features + [target])\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    X_shadow = X.copy()\n",
    "\n",
    "    # We will determine optimal parameters on a random sample. We'll use these parameters to construct a new model for each iteration.\n",
    "    # Determining the hyperparameters takes time, so we'll do it once, and use these parameters to construct our models.\n",
    "    for f in features:\n",
    "        X_shadow[f + '_shadow'] = create_shadow_feature(X_shadow[f])\n",
    "\n",
    "    # These are the features, categorical features, and categorical feature indices that we will use once the shadow features are added.\n",
    "    shadow_features = features + [f + '_shadow' for f in features]\n",
    "    shadow_cat_features = cat_features + [x + '_shadow' for x in cat_features]\n",
    "\n",
    "    X_train, X_test = train_test_split(X_shadow, train_size=sample, shuffle=True)\n",
    "\n",
    "    params_0 = params_tune(X=X_train,\n",
    "                           target=target,\n",
    "                           features=shadow_features,\n",
    "                           cat_features=shadow_cat_features,\n",
    "                           num_boost_rounds=num_boost_rounds,\n",
    "                           params=starting_params,\n",
    "                           nfolds=nfolds,\n",
    "                           verbose_eval=False,\n",
    "                           sample = sample)\n",
    "\n",
    "    dtrain = lgbm.Dataset(data=X_train[features],\n",
    "                          label=X_train[target],\n",
    "                          feature_name=features,\n",
    "                          categorical_feature= \n",
    "                         )\n",
    "\n",
    "    current_model = lgbm.train(params=params_0,\n",
    "                               train_set=dtrain,\n",
    "                               num_boost_round=num_boost_rounds,\n",
    "                               valid_sets=dval,\n",
    "                               feature_name=shadow_features,\n",
    "                               early_stopping_rounds=int(0.05*num_boost_rounds),\n",
    "                               verbose_eval=100)\n",
    "\n",
    "    shap_values = shap.TreeExplainer(current_model).shap_values(X_shap)\n",
    "    shap_values_df = pd.DataFrame(data = shap_values[1], columns = shadow_features)\n",
    "    shap_values_df[target] = np.where(y_shap == 1, 1, 0)\n",
    "\n",
    "    print('Missing targets in y_shap = ', sum(y_shap.isna()))\n",
    "    print('Missing targets in y_shap = ', sum(y_shap))\n",
    "    print('Missing targets in shap_values_df = ', sum(shap_values_df['target'].isna()))\n",
    "    print('Missing targets in shap_values_df = ', sum(shap_values_df['target']))\n",
    "\n",
    "    #print(results.shape)\n",
    "    #print(np.abs(shap_values[0]).mean(0))\n",
    "    print(results_df.shape)\n",
    "    #print(results_df.sample(10))\n",
    "    print(shap_values_df.shape)\n",
    "    print([x for x in list(results_df) if x not in list(shap_values_df)])\n",
    "    print([x for x in list(shap_values_df) if x not in list(results_df)])\n",
    "    #print(shap_values_df.sample(10))\n",
    "    results_df = pd.concat([results_df, shap_values_df], axis = 0)\n",
    "\n",
    "cols_to_keep = []\n",
    "n = results_df.shape[0]\n",
    "for col in features:\n",
    "    x = np.sum(np.where((results_df[target] == 0) & (results_df[col] < results_df[col + '_shadow']), 1,\n",
    "                    np.where((results_df[target] == 1) & (results_df[col] > results_df[col + '_shadow']), 1, 0)))\n",
    "    p_val = binom_test(x=x, n=n, p=0.5, alternative='greater')\n",
    "    if p_val <= 0.05:\n",
    "        cols_to_keep += [col]\n",
    "\n",
    "return cols_to_keep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_tune(X:pd.DataFrame, target:str, features:list, cat_features:list, num_boost_rounds:int, params:dict, nfolds:int=3, verbose_eval:bool = True, sample:float = 0.5) -> dict:\n",
    "    \n",
    "    X = X.sample(frac=sample)\n",
    "    \n",
    "    dtrain = lgbm.Dataset(data=X[features],\n",
    "                          label=X[target],\n",
    "                          feature_name=features)\n",
    "    \n",
    "    model = opt.integration.lightgbm.LightGBMTunerCV(params = params,\n",
    "                                                     train_set = dtrain,\n",
    "                                                     num_boost_round = num_boost_rounds,\n",
    "                                                     nfold = nfolds,\n",
    "                                                     #stratified = True,\n",
    "                                                     shuffle = True,\n",
    "                                                     feature_name=features,\n",
    "                                                     categorical_feature=cat_features,\n",
    "                                                     early_stopping_rounds=0.05*num_boost_rounds,\n",
    "                                                     verbose_eval = verbose_eval,\n",
    "                                                     seed = 51)\n",
    "    model.run()\n",
    "    print(model.best_score)\n",
    "    return model.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to conver the categorical variables to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [x for x in list(data) if 'cat' in x]:\n",
    "    data[f] = categorical_convert(data[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'root_mean_squared_error',\n",
    "          'verbosity': -1,\n",
    "          'boosting_type': 'gbdt',\n",
    "          'learning_rate': 0.25}\n",
    "\n",
    "params_0 = params_tune(X=data,\n",
    "                       target='target',\n",
    "                       features=[x for x in list(data) if ('cat' in x) or ('cont' in x)],\n",
    "                       cat_features=[x for x in list(data) if ('cat' in x)],\n",
    "                       num_boost_rounds=5000,\n",
    "                       params=params,\n",
    "                       nfolds=3,\n",
    "                       verbose_eval=False,\n",
    "                       sample = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_shap(X:pd.DataFrame, features:list, cat_features:list, target:str, starting_params:dict, num_boost_rounds:int=10000, sample:float = 0.7, num_iter:int = 3) -> list:\n",
    "                            \n",
    "    # This is the data frame that we will use to store our SHAP value information\n",
    "    results_df = pd.DataFrame(columns = shadow_features + [target])\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        X_shadow = X.copy()\n",
    "        \n",
    "        # We will determine optimal parameters on a random sample. We'll use these parameters to construct a new model for each iteration.\n",
    "        # Determining the hyperparameters takes time, so we'll do it once, and use these parameters to construct our models.\n",
    "        for f in features:\n",
    "            X_shadow[f + '_shadow'] = create_shadow_feature(X_shadow[f])\n",
    "        \n",
    "        # These are the features, categorical features, and categorical feature indices that we will use once the shadow features are added.\n",
    "        shadow_features = features + [f + '_shadow' for f in features]\n",
    "        shadow_cat_features = cat_features + [x + '_shadow' for x in cat_features]\n",
    "        \n",
    "        X_train, X_test = train_test_split(X_shadow, train_size=sample, shuffle=True)\n",
    "        \n",
    "        params_0 = params_tune(X=X_train,\n",
    "                               target=target,\n",
    "                               features=shadow_features,\n",
    "                               cat_features=shadow_cat_features,\n",
    "                               num_boost_rounds=num_boost_rounds,\n",
    "                               params=starting_params,\n",
    "                               nfolds=nfolds,\n",
    "                               verbose_eval=False,\n",
    "                               sample = sample)\n",
    "        \n",
    "        dtrain = lgbm.Dataset(data=X_train[features],\n",
    "                              label=X_train[target],\n",
    "                              feature_name=features,\n",
    "                              categorical_feature= \n",
    "                             )\n",
    "        \n",
    "        current_model = lgbm.train(params=params_0,\n",
    "                                   train_set=dtrain,\n",
    "                                   num_boost_round=num_boost_rounds,\n",
    "                                   valid_sets=dval,\n",
    "                                   feature_name=shadow_features,\n",
    "                                   early_stopping_rounds=int(0.05*num_boost_rounds),\n",
    "                                   verbose_eval=100)\n",
    "        \n",
    "        shap_values = shap.TreeExplainer(current_model).shap_values(X_shap)\n",
    "        shap_values_df = pd.DataFrame(data = shap_values[1], columns = shadow_features)\n",
    "        shap_values_df[target] = np.where(y_shap == 1, 1, 0)\n",
    "        \n",
    "        print('Missing targets in y_shap = ', sum(y_shap.isna()))\n",
    "        print('Missing targets in y_shap = ', sum(y_shap))\n",
    "        print('Missing targets in shap_values_df = ', sum(shap_values_df['target'].isna()))\n",
    "        print('Missing targets in shap_values_df = ', sum(shap_values_df['target']))\n",
    "        \n",
    "        #print(results.shape)\n",
    "        #print(np.abs(shap_values[0]).mean(0))\n",
    "        print(results_df.shape)\n",
    "        #print(results_df.sample(10))\n",
    "        print(shap_values_df.shape)\n",
    "        print([x for x in list(results_df) if x not in list(shap_values_df)])\n",
    "        print([x for x in list(shap_values_df) if x not in list(results_df)])\n",
    "        #print(shap_values_df.sample(10))\n",
    "        results_df = pd.concat([results_df, shap_values_df], axis = 0)\n",
    "    \n",
    "    cols_to_keep = []\n",
    "    n = results_df.shape[0]\n",
    "    for col in features:\n",
    "        x = np.sum(np.where((results_df[target] == 0) & (results_df[col] < results_df[col + '_shadow']), 1,\n",
    "                        np.where((results_df[target] == 1) & (results_df[col] > results_df[col + '_shadow']), 1, 0)))\n",
    "        p_val = binom_test(x=x, n=n, p=0.5, alternative='greater')\n",
    "        if p_val <= 0.05:\n",
    "            cols_to_keep += [col]\n",
    "            \n",
    "    return cols_to_keep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\tvalid_0's auc: 0.615545\n",
      "[200]\tvalid_0's auc: 0.620539\n",
      "[300]\tvalid_0's auc: 0.622757\n",
      "[400]\tvalid_0's auc: 0.621391\n",
      "[500]\tvalid_0's auc: 0.62185\n",
      "[600]\tvalid_0's auc: 0.622995\n",
      "[700]\tvalid_0's auc: 0.624099\n",
      "[800]\tvalid_0's auc: 0.622831\n",
      "[900]\tvalid_0's auc: 0.625327\n",
      "[1000]\tvalid_0's auc: 0.624036\n",
      "[1100]\tvalid_0's auc: 0.625355\n",
      "[1200]\tvalid_0's auc: 0.626502\n",
      "[1300]\tvalid_0's auc: 0.626611\n",
      "[1400]\tvalid_0's auc: 0.626031\n",
      "[1500]\tvalid_0's auc: 0.626124\n",
      "[1600]\tvalid_0's auc: 0.625573\n",
      "[1700]\tvalid_0's auc: 0.625923\n",
      "[1800]\tvalid_0's auc: 0.625734\n",
      "[1900]\tvalid_0's auc: 0.626323\n",
      "[2000]\tvalid_0's auc: 0.6267\n",
      "[2100]\tvalid_0's auc: 0.627149\n",
      "[2200]\tvalid_0's auc: 0.626988\n",
      "[2300]\tvalid_0's auc: 0.626752\n",
      "[2400]\tvalid_0's auc: 0.626532\n",
      "[2500]\tvalid_0's auc: 0.625905\n",
      "[2600]\tvalid_0's auc: 0.626303\n",
      "[2700]\tvalid_0's auc: 0.626939\n",
      "[2800]\tvalid_0's auc: 0.62694\n",
      "[2900]\tvalid_0's auc: 0.626564\n",
      "[3000]\tvalid_0's auc: 0.627321\n",
      "Early stopping, best iteration is:\n",
      "[2054]\tvalid_0's auc: 0.627538\n"
     ]
    }
   ],
   "source": [
    "features_to_keep = feature_selection_shap(X=data,\n",
    "                                          features=features,\n",
    "                                          target='target',\n",
    "                                          params=params_0,\n",
    "                                          num_boost_rounds=20000,\n",
    "                                          sample=0.7,\n",
    "                                          num_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resp']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgbm.Dataset(data=data[[x for x in features if x not in ['resp','weight']]],\n",
    "                          label=data['target'],\n",
    "                          feature_name=[x for x in features if x not in ['resp','weight']])\n",
    "\n",
    "test_params['metric'] = 'auc'\n",
    "test_model = lgbm.cv(params = test_params,\n",
    "                        train_set = dtrain,\n",
    "                        num_boost_round=10000,\n",
    "                        nfold=5,\n",
    "                        stratified=True,\n",
    "                        shuffle=True,\n",
    "                        metrics='auc',\n",
    "                        feature_name=[x for x in features if x not in ['resp','weight']],\n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trees_tune(X:pd.DataFrame, target:str, features:list, num_boost_rounds:int, params:dict, learning_rate:float = 0.01, num_iter:int = 10, train_sample:float = 0.7, verbose_eval:int = 0):\n",
    "    \n",
    "    cat_features_indices = [i for i in range(len(features)) if features[i] in cat_features]\n",
    "    params['learning_rate'] = learning_rate\n",
    "    best_iter_list = []\n",
    "        \n",
    "    for i in range(num_iter):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X[features], X[target], train_size = train_sample, shuffle = True, stratify = X[target])\n",
    "        if len(cat_features) == 0:\n",
    "            X_train_smote, y_train_smote = SMOTE(random_state=51).fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            X_train_smote, y_train_smote = SMOTENC(categorical_features=cat_features_indices, random_state=51).fit_resample(X_train, y_train)\n",
    "        \n",
    "        del(X_train, y_train)\n",
    "        gc.collect()\n",
    "        \n",
    "        dtrain = lgbm.Dataset(data = X_train_smote,\n",
    "                                label = y_train_smote,\n",
    "                                feature_name = features,\n",
    "                                categorical_feature = cat_features)\n",
    "        \n",
    "        dval = lgbm.Dataset(data = X_val,\n",
    "                            label = y_val,\n",
    "                            feature_name = features,\n",
    "                            categorical_feature = cat_features)\n",
    "        \n",
    "        del(X_train_smote, y_train_smote, X_val, y_val)\n",
    "        gc.collect()\n",
    "        \n",
    "        current_model = lgbm.train(params=params,\n",
    "                                    train_set=dtrain,\n",
    "                                    num_boost_round=num_boost_rounds,\n",
    "                                    valid_sets=[dval],\n",
    "                                    feature_name=features,\n",
    "                                    categorical_feature=cat_features,\n",
    "                                    early_stopping_rounds=1000,\n",
    "                                    verbose_eval=0)\n",
    "\n",
    "        best_iter_current = current_model.best_iteration\n",
    "        best_iter_list += [best_iter_current]\n",
    "        \n",
    "        print(current_model.best_score)\n",
    "\n",
    "    return int(np.nanmean(best_iter_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns = ['target', 'param', 'val'])\n",
    "\n",
    "params = {'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.25,\n",
    "            'is_unbalanced': False}\n",
    "    \n",
    "for target in targets:\n",
    "    train = train_all.copy()\n",
    "    augmented_vals = augmenting_vals(train, target, num = 20)\n",
    "    train = pd.concat([train, augmented_vals], axis = 0, ignore_index = True)\n",
    "    best_params = params_tune(X=train, target=target, features=features, cat_features=cat_features, num_boost_rounds=10000, params=params)\n",
    "    \n",
    "    features_to_keep = feature_selection_shap(X=train[features], y=train[target], params=best_params, features=features, cat_features=cat_features, sample=0.5, num_boost_rounds=10000, num_iter = 3, verbose_eval = 0)\n",
    "    cat_features_to_keep = [x for x in cat_features if x in features_to_keep]\n",
    "    \n",
    "    best_params['features'] = features_to_keep\n",
    "    best_params['cat_features'] = cat_features_to_keep\n",
    "    \n",
    "    num_trees = num_trees_tune(X=train, target=target, features=features_to_keep, cat_features=cat_features_to_keep, num_boost_rounds=50000, params=best_params, learning_rate = 0.01, num_iter = 10, train_sample = 0.7, verbose_eval = 0)\n",
    "    best_params['num_trees'] = int(1.1*num_trees)\n",
    "    \n",
    "    for key in best_params.keys():\n",
    "        results_df = pd.concat([results_df, pd.DataFrame(data=[[target, key, best_params[key]]], columns=['target', 'param', 'val'])], axis=0, ignore_index=True)\n",
    "    print(results_df.tail(10))\n",
    "    print(target, ' completed. ', len(targets) - targets.index(target) - 1, ' to go...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
